This repository contains the code used for the project for the 2025 course of Deep Natural Language Processing for the group "Plumbers".
We leave the outputs generated by the code in the Colab Notebooks, so that they can be verified without running the code, if desired.
All the expected runtimes are intended for using Colab with a T4 GPU. 

# Content

## models
This folder contains the model(s) we used and/or fine-tuned, in order to avoid re-training or re-finetuning the models every time.
```fine_tuned_keybert_model_0_2.zip``` is a zip file containing the model all-miniLM-L6-v2, fine-tuned using 20% of the Krapvin2009 dataset.

## notebooks
This folder contains all the .ipynb files used to develop the project. The folders "First-extension" and "Second-extension" contain the file used to work on the extensions for the base model.

- ```hyperparameter_tuning.ipynb``` is used for the evaluation of two hyperparameters: n_gram range [(1,2), (1,3)] and distance metric [cosine_similarity, maxsum, mmr]. All three distance metrics are evaluated in the file, but to change the n_gram length it is necessary to manually modify the variable clearly visible in the code.
  Total expected runtime: about 1h for n_gram=(1,2), 2h for n_gram=(1,3)

- ```base_model_evaluation.ipynb``` evaluates the base version of KeyBERT on the Nguyen2007 and SemEval2010 datasets. The file can be launched as it is.
  Total expected runtime: about 15m

### First-extension
Contains the file used to develop the first extension

- ```extension1_fine_tune_model.ipynb``` contains the fine-tuning process of all-MiniLM-L6-v2. The fine-tuned model has been manually downloaded and uploaded on github to be used in the other files.
  Total expected runtime: about 25m
  
- ```fine_tuned_model_evaluation.ipynb``` contains the evaluation of the fine-tuned model in the same way the base model has been evaluated. Can be launched as it is since it retrieves the model from GitHub.
  Total expected runtime: about 15m
  
- ```scibert_evaluation.ipynb``` contains the evaluation of SciBERT model in the same way the base model has been evaluated. Can be launched as it is since it retrieves the model from HuggingFace.
  Total expected runtime: about 15m

  ### Second-extension

  
