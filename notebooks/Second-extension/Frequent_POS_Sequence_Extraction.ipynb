{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["eLBEqntdPk62","fNYsnAtSXXS6"],"authorship_tag":"ABX9TyOsAKCIZCinMLTCIIG8dcH7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# DATABASE EXTRACTION and PROCESSING"],"metadata":{"id":"eLBEqntdPk62"}},{"cell_type":"code","source":["import os\n","import requests\n","import zipfile\n","import time\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"Oj3Rk0eFVDRz","executionInfo":{"status":"ok","timestamp":1739177894457,"user_tz":-60,"elapsed":1633,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"hP9slT9JPENL","executionInfo":{"status":"ok","timestamp":1739177894465,"user_tz":-60,"elapsed":3,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"outputs":[],"source":["# Define the URL for the dataset repository and the local storage directory\n","data_url = \"https://github.com/LIAAD/KeywordExtractor-Datasets/archive/refs/heads/master.zip\"\n","local_zip_path = \"datasets.zip\"\n","unzip_dir = \"KeywordExtractor-Datasets\""]},{"cell_type":"code","source":["# Step 1: Download the dataset repository\n","def download_datasets():\n","    print(\"Downloading datasets...\")\n","    response = requests.get(data_url)\n","    if response.status_code == 200:\n","        with open(local_zip_path, \"wb\") as file:\n","            file.write(response.content)\n","        print(\"Datasets downloaded successfully.\")\n","    else:\n","        print(f\"Failed to download datasets. Status code: {response.status_code}\")\n","        exit(1)"],"metadata":{"id":"m-nw69U-AmwF","executionInfo":{"status":"ok","timestamp":1739177894469,"user_tz":-60,"elapsed":2,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Step 2: Extract the downloaded zip file\n","def extract_datasets():\n","    print(\"Extracting datasets...\")\n","    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(unzip_dir)\n","    print(\"Datasets extracted successfully.\")"],"metadata":{"id":"t9odQOW3AsNe","executionInfo":{"status":"ok","timestamp":1739177894474,"user_tz":-60,"elapsed":2,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Step 3: Extract individual dataset zips\n","def extract_inner_zips():\n","    datasets_path = os.path.join(unzip_dir, \"KeywordExtractor-Datasets-master/datasets\")\n","    for file in os.listdir(datasets_path):\n","        if file.endswith(\".zip\"):\n","            zip_path = os.path.join(datasets_path, file)\n","            extract_path = os.path.join(datasets_path, file.replace(\".zip\", \"\"))\n","            if not os.path.exists(extract_path):\n","                print(f\"Extracting {file}...\")\n","                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","                    zip_ref.extractall(extract_path)\n","                print(f\"Extracted {file} to {extract_path}.\")"],"metadata":{"id":"Cb-eAgdqAvWQ","executionInfo":{"status":"ok","timestamp":1739177894479,"user_tz":-60,"elapsed":3,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# PROCESSING WITHOUT SAMPLING\n","# Step 4: Process a specific dataset and convert it into a usable format\n","def process_dataset(dataset_name):\n","    dataset_path = os.path.join(unzip_dir, f\"KeywordExtractor-Datasets-master/datasets/{dataset_name}/{dataset_name}\")\n","\n","    # Check if dataset directory exists\n","    if not os.path.exists(dataset_path):\n","        print(f\"Dataset {dataset_name} not found.\")\n","        return None\n","\n","    docs_folder = os.path.join(dataset_path, \"docsutf8\")\n","    keys_folder = os.path.join(dataset_path, \"keys\")\n","\n","    if not os.path.exists(docs_folder) or not os.path.exists(keys_folder):\n","        print(f\"Required folders (docsutf8, keys) are missing in {dataset_name}.\")\n","        return None\n","\n","    # Load documents and keywords\n","    print(f\"Processing dataset: {dataset_name}\")\n","    documents = []\n","    keywords = []\n","\n","    for doc_file in sorted(os.listdir(docs_folder)):\n","        doc_path = os.path.join(docs_folder, doc_file)\n","        if doc_file.endswith(\".txt\"):\n","            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n","                documents.append(f.read().strip())\n","\n","    for key_file in sorted(os.listdir(keys_folder)):\n","        key_path = os.path.join(keys_folder, key_file)\n","        if key_file.endswith(\".key\"):\n","            with open(key_path, \"r\", encoding=\"utf-8\") as f:\n","                keywords.append(f.read().strip().split(','))\n","\n","    # Combine documents and keywords into a DataFrame\n","    data = pd.DataFrame({\"document\": documents, \"keywords\": keywords})\n","    return data"],"metadata":{"id":"NohTRCgrBBTb","executionInfo":{"status":"ok","timestamp":1739177894489,"user_tz":-60,"elapsed":8,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# PROCESSING WITH SAMPLING\n","\n","def process_dataset(dataset_name, sample_fraction=0.2, random_seed=64):\n","    dataset_path = os.path.join(unzip_dir, f\"KeywordExtractor-Datasets-master/datasets/{dataset_name}/{dataset_name}\")\n","\n","    # Check if dataset directory exists\n","    if not os.path.exists(dataset_path):\n","        print(f\"Dataset {dataset_name} not found.\")\n","        return None\n","\n","    docs_folder = os.path.join(dataset_path, \"docsutf8\")\n","    keys_folder = os.path.join(dataset_path, \"keys\")\n","\n","    if not os.path.exists(docs_folder) or not os.path.exists(keys_folder):\n","        print(f\"Required folders (docsutf8, keys) are missing in {dataset_name}.\")\n","        return None\n","\n","    # Get list of all document files\n","    doc_files = sorted([f for f in os.listdir(docs_folder) if f.endswith(\".txt\")])\n","\n","    # Set random seed and sample file indices\n","    np.random.seed(random_seed)\n","    sample_size = int(len(doc_files) * sample_fraction)\n","    sampled_indices = np.random.choice(len(doc_files), size=sample_size, replace=False)\n","\n","    print(f\"Original dataset size: {len(doc_files)}\")\n","    print(f\"Sampled dataset size: {sample_size}\")\n","\n","    # Load only sampled documents and their corresponding keywords\n","    documents = []\n","    keywords = []\n","\n","    for idx in sorted(sampled_indices):\n","        # Get document\n","        doc_file = doc_files[idx]\n","        doc_path = os.path.join(docs_folder, doc_file)\n","        with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n","            documents.append(f.read().strip())\n","\n","        # Get corresponding keywords\n","        key_file = doc_file.replace('.txt', '.key')\n","        key_path = os.path.join(keys_folder, key_file)\n","        with open(key_path, \"r\", encoding=\"utf-8\") as f:\n","            keywords.append(f.read().strip().split(','))\n","\n","    # Combine documents and keywords into a DataFrame\n","    data = pd.DataFrame({\"document\": documents, \"keywords\": keywords})\n","    return data"],"metadata":{"id":"0BYbwaRAUwUU","executionInfo":{"status":"ok","timestamp":1739177894516,"user_tz":-60,"elapsed":24,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Step 4: Save processed data to a CSV file\n","def save_to_csv(data, output_path):\n","    print(f\"Saving processed data to {output_path}...\")\n","    data.to_csv(output_path, index=False, encoding=\"utf-8\")\n","    print(\"Data saved successfully.\")"],"metadata":{"id":"42uv-e45BJEU","executionInfo":{"status":"ok","timestamp":1739177894523,"user_tz":-60,"elapsed":2,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Main execution\n","if __name__ == \"__main__\":\n","    download_datasets()\n","    extract_datasets()\n","    extract_inner_zips()\n","\n","    # Example: Process the \"Krapivin2009\" dataset\n","    dataset_name = \"Krapivin2009\"\n","    processed_data = process_dataset(dataset_name)\n","\n","    if processed_data is not None:\n","        output_csv = f\"{dataset_name}_processed.csv\"\n","        save_to_csv(processed_data, output_csv)\n","        #in the format documents,keywords\n","        #keywords are separated either by commas or \\n\n","\n","    # Clean up downloaded zip file\n","    if os.path.exists(local_zip_path):\n","        os.remove(local_zip_path)\n","        print(\"Cleaned up temporary files.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c415be42-d7ce-4603-e79c-004ed00b8cd2","collapsed":true,"id":"rjardzbIk5g_","executionInfo":{"status":"ok","timestamp":1739177917281,"user_tz":-60,"elapsed":22749,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading datasets...\n","Datasets downloaded successfully.\n","Extracting datasets...\n","Datasets extracted successfully.\n","Extracting theses100.zip...\n","Extracted theses100.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/theses100.\n","Extracting SemEval2010.zip...\n","Extracted SemEval2010.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/SemEval2010.\n","Extracting SemEval2017.zip...\n","Extracted SemEval2017.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/SemEval2017.\n","Extracting kdd.zip...\n","Extracted kdd.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/kdd.\n","Extracting cacic.zip...\n","Extracted cacic.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/cacic.\n","Extracting PubMed.zip...\n","Extracted PubMed.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/PubMed.\n","Extracting 500N-KPCrowd-v1.1.zip...\n","Extracted 500N-KPCrowd-v1.1.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/500N-KPCrowd-v1.1.\n","Extracting wiki20.zip...\n","Extracted wiki20.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/wiki20.\n","Extracting citeulike180.zip...\n","Extracted citeulike180.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/citeulike180.\n","Extracting wicc.zip...\n","Extracted wicc.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/wicc.\n","Extracting fao30.zip...\n","Extracted fao30.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/fao30.\n","Extracting fao780.zip...\n","Extracted fao780.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/fao780.\n","Extracting pak2018.zip...\n","Extracted pak2018.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/pak2018.\n","Extracting Inspec.zip...\n","Extracted Inspec.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/Inspec.\n","Extracting www.zip...\n","Extracted www.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/www.\n","Extracting Krapivin2009.zip...\n","Extracted Krapivin2009.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/Krapivin2009.\n","Extracting Schutz2008.zip...\n","Extracted Schutz2008.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/Schutz2008.\n","Extracting 110-PT-BN-KP.zip...\n","Extracted 110-PT-BN-KP.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/110-PT-BN-KP.\n","Extracting Nguyen2007.zip...\n","Extracted Nguyen2007.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/Nguyen2007.\n","Extracting WikiNews.zip...\n","Extracted WikiNews.zip to KeywordExtractor-Datasets/KeywordExtractor-Datasets-master/datasets/WikiNews.\n","Original dataset size: 2304\n","Sampled dataset size: 460\n","Saving processed data to Krapivin2009_processed.csv...\n","Data saved successfully.\n","Cleaned up temporary files.\n"]}]},{"cell_type":"code","source":["# Load the CSV file\n","file_path = \"./Krapivin2009_processed.csv\"  # Update with the correct path\n","data = pd.read_csv(file_path)\n","\n","\"\"\"\n","# Extract documents and keywords\n","documents = data['document']  # This is a pandas Series of text documents\n","keywords = data['keywords']  # This is a pandas Series of keyword strings\n","\"\"\"\n","\n","# Extract documents and keywords\n","# save them in pandas dataframe in the 'text' column\n","documents = pd.DataFrame({'text': data['document']})\n","keywords = pd.DataFrame({'text': data['keywords']})\n","\n","# Convert keywords from string representation to Python lists\n","keywords['text'] = keywords['text'].apply(lambda x: eval(x))  # Use `eval` to parse strings into lists if necessary\n","#Now it is a list containing only one long string in the format ['Keyword1\\nKeyword2\\nKeyword3....']\n","# keywords = keywords.apply(lambda x: x[0].lower()) #only one element in the list\n","#Now x is a string lowercase\n","keywords['text'] = keywords['text'].apply(lambda x: x[0].split('\\n'))\n","#now keywords is a list of lowercase strings"],"metadata":{"collapsed":true,"id":"Dao9TY5sk9xm","executionInfo":{"status":"ok","timestamp":1739177917646,"user_tz":-60,"elapsed":354,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# PART-OF-SPEECH TAGGING [documents]"],"metadata":{"id":"fNYsnAtSXXS6"}},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"qAX5sXkTXWWv","executionInfo":{"status":"ok","timestamp":1739177994592,"user_tz":-60,"elapsed":8661,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# the function compiute part-of-speech tagging using spacy on a text and return a list of pair (word, pos)\n","\n","def pos_tag_document(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","\n","    # Store and return tagged words with their parts of speech\n","    # each token object has a token.texta and a token.pos_ attribute here are saved in a list of pairs\n","    tagged_words = [(token.text, token.pos_) for token in doc]\n","\n","    return tagged_words"],"metadata":{"id":"Ar3mXJ0rVCrH","executionInfo":{"status":"ok","timestamp":1739177994701,"user_tz":-60,"elapsed":98,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# main\n","\n","start = time.time()\n","documents['pos'] = documents['text'].apply(pos_tag_document)\n","end = time.time()\n","print(f'part-of-speech tagging of the documents {end-start}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJNbYn_svzNY","outputId":"42e4be8e-c1a4-41f9-ebb0-cf696941ea2c","executionInfo":{"status":"ok","timestamp":1739179353492,"user_tz":-60,"elapsed":1358789,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["part-of-speech tagging of the documents 1358.8119547367096\n"]}]},{"cell_type":"markdown","source":["# PART-OF-SPEECH ASSOCIATION [keywords]"],"metadata":{"id":"Ss3yoHv5bG3m"}},{"cell_type":"code","source":["from collections import Counter\n","from typing import List, Tuple\n","import re"],"metadata":{"id":"Y40hqthnb4-Q","executionInfo":{"status":"ok","timestamp":1739179398693,"user_tz":-60,"elapsed":45,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# it recive the pos tagging of a document and the list of keyword of that document and return\n","# the pos of the keywords obtained by collecting the pos taggs of all the occurrence of the keyword in the\n","# text and keeping the most common\n","\n","def pos_of_keywords(keywords, text_pos):\n","\n","  key_pos = []\n","  # lower the word of the text to have better comparison\n","  text = [text.lower() for text, _ in text_pos]\n","  pos = [pos for _, pos in text_pos]\n","\n","\n","  for key in keywords:\n","        # PREVIOUS : Split the keywords in single words and lower them for better comparison\n","        # key_tokens = key.lower().split()\n","\n","        # using findall regular exression I split the string and consider \"-\" and \"/\" as a string\n","        # as well sonce the POS includes them\n","        # - and / because are the recurring one in the keyword based on a sample of the kewords not found\n","        key_tokens = re.findall(r'\\w+|[-/()]', key.lower())\n","\n","        # Find all occurrences of the keyword\n","        occurrences = []\n","        for i in range(len(text_pos) - len(key_tokens) + 1):\n","            if text[i:i+len(key_tokens)] == key_tokens:\n","                # Extract POS sequence for this occurrence\n","                pos_sequence = [pos[j] for j in range(i, i+len(key_tokens))]\n","                occurrences.append(tuple(pos_sequence)) # tuple because counter require hushable type\n","\n","        # If occurrences found, use the most common POS sequence\n","        if occurrences:\n","            # Counter collect occurrences and # of occurrence ((('ADJ','ADJ'), 3), ('NOUN', 'ADJ'), 1)\n","            # most_common(n) selects a list of the n most common occurrences\n","            most_common_pos = Counter(occurrences).most_common(1)[0][0]\n","            key_pos.append(most_common_pos)\n","        else:\n","            key_pos.append(None)\n","\n","  return(key_pos)"],"metadata":{"id":"NTEA_25saDfH","executionInfo":{"status":"ok","timestamp":1739179398705,"user_tz":-60,"elapsed":4,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# execute pos_of_keywords on every row of the dataframes\n","\n","start = time.time()\n","keywords['pos'] = keywords.apply(lambda x: pos_of_keywords(x['text'], documents['pos'][x.name]), axis=1)\n","end = time.time()\n","print(f'part-of-speech tagging of the keywords {end-start}') # by associaton of the keywords to their occurrences in the text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmQt-dyW0aod","outputId":"8e4f00f7-5934-45e5-e5df-e7094794415e","executionInfo":{"status":"ok","timestamp":1739179405180,"user_tz":-60,"elapsed":6477,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["part-of-speech tagging of the keywords 6.480237007141113\n"]}]},{"cell_type":"code","source":["# TO DO : decide how many pos sequence to select\n","\n","def most_common_pos_sequences(pos_list, n=50):\n","  most_common_pos = Counter(pos_list).most_common(n)\n","  # print(most_common_pos)\n","  # print(len(most_common_pos))\n","  return [mcs[0] for mcs in most_common_pos if mcs[0] is not None and len(mcs[0])<=3]\n","\n","# have all the keyword pos sequence in a single list\n","flattened = tuple(keywords['pos'].explode().tolist())\n","\n","accepted_pos_sequences = most_common_pos_sequences(flattened)\n","\n","# Forse poche poarole chiave\n","# Da eliminare quelle da 4\n","# Tutte ragionevoli e ben distribuite da 50"],"metadata":{"id":"l5kDcuPg1-PP","executionInfo":{"status":"ok","timestamp":1739179539918,"user_tz":-60,"elapsed":61,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49bf4e3e-0fa3-4d11-d36b-1a56ed7937eb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[('NOUN', 'NOUN'), ('ADJ', 'NOUN'), ('NOUN',), ('PROPN', 'PROPN'), ('ADJ', 'NOUN', 'NOUN'), ('PROPN',), ('PROPN', 'NOUN'), ('VERB', 'NOUN'), ('NOUN', 'VERB'), ('NOUN', 'NOUN', 'NOUN'), ('PROPN', 'PROPN', 'PROPN'), ('VERB',), ('ADJ', 'ADJ', 'NOUN'), ('VERB', 'NOUN', 'NOUN'), ('ADJ',), ('NOUN', 'PUNCT', 'NOUN'), ('NOUN', 'PROPN'), ('PROPN', 'NOUN', 'NOUN'), ('ADJ', 'PROPN'), ('NOUN', 'ADP', 'NOUN'), ('ADJ', 'PROPN', 'NOUN'), ('PROPN', 'ADP', 'PROPN'), ('PROPN', 'ADJ', 'NOUN'), ('NOUN', 'VERB', 'NOUN'), ('PROPN', 'VERB'), ('ADJ', 'NOUN', 'VERB'), ('NOUN', 'ADJ', 'NOUN'), ('NOUN', 'PUNCT', 'VERB'), ('VERB', 'PROPN'), ('NOUN', 'PROPN', 'NOUN'), ('PROPN', 'PUNCT', 'NOUN'), ('X', 'X', 'NOUN'), ('PROPN', 'PROPN', 'NOUN'), ('PROPN', 'PUNCT', 'PROPN')]\n","34\n"]}]},{"cell_type":"code","source":["selection = accepted_pos_sequences[:20]\n","print(selection)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNt37ggrLfIv","executionInfo":{"status":"ok","timestamp":1739180152720,"user_tz":-60,"elapsed":32,"user":{"displayName":"Sara Mola","userId":"13852974521966023399"}},"outputId":"24c93e97-b08b-4196-e9f4-3655adc532d6"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[('NOUN', 'NOUN'), ('ADJ', 'NOUN'), ('NOUN',), ('PROPN', 'PROPN'), ('ADJ', 'NOUN', 'NOUN'), ('PROPN',), ('PROPN', 'NOUN'), ('VERB', 'NOUN'), ('NOUN', 'VERB'), ('NOUN', 'NOUN', 'NOUN'), ('PROPN', 'PROPN', 'PROPN'), ('VERB',), ('ADJ', 'ADJ', 'NOUN'), ('VERB', 'NOUN', 'NOUN'), ('ADJ',), ('NOUN', 'PUNCT', 'NOUN'), ('NOUN', 'PROPN'), ('PROPN', 'NOUN', 'NOUN'), ('ADJ', 'PROPN'), ('NOUN', 'ADP', 'NOUN')]\n"]}]}]}