{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eLBEqntdPk62",
        "fNYsnAtSXXS6"
      ],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s319848/DNLP-project-2025/blob/main/notebooks/Second-extension/Frequent_POS_Sequence_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATABASE EXTRACTION and PROCESSING"
      ],
      "metadata": {
        "id": "eLBEqntdPk62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Oj3Rk0eFVDRz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.310159Z",
          "iopub.execute_input": "2025-02-10T14:34:37.310464Z",
          "iopub.status.idle": "2025-02-10T14:34:37.314245Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.310438Z",
          "shell.execute_reply": "2025-02-10T14:34:37.313440Z"
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL for the dataset repository and the local storage directory\n",
        "data_url = \"https://github.com/LIAAD/KeywordExtractor-Datasets/archive/refs/heads/master.zip\"\n",
        "local_zip_path = \"datasets.zip\"\n",
        "unzip_dir = \"KeywordExtractor-Datasets\""
      ],
      "metadata": {
        "id": "hP9slT9JPENL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.321558Z",
          "iopub.execute_input": "2025-02-10T14:34:37.321791Z",
          "iopub.status.idle": "2025-02-10T14:34:37.335493Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.321772Z",
          "shell.execute_reply": "2025-02-10T14:34:37.334777Z"
        }
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download the dataset repository\n",
        "def download_datasets():\n",
        "    print(\"Downloading datasets...\")\n",
        "    response = requests.get(data_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_zip_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(\"Datasets downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download datasets. Status code: {response.status_code}\")\n",
        "        exit(1)"
      ],
      "metadata": {
        "id": "m-nw69U-AmwF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.336690Z",
          "iopub.execute_input": "2025-02-10T14:34:37.337021Z",
          "iopub.status.idle": "2025-02-10T14:34:37.354082Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.336990Z",
          "shell.execute_reply": "2025-02-10T14:34:37.353477Z"
        }
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract the downloaded zip file\n",
        "def extract_datasets():\n",
        "    print(\"Extracting datasets...\")\n",
        "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(unzip_dir)\n",
        "    print(\"Datasets extracted successfully.\")"
      ],
      "metadata": {
        "id": "t9odQOW3AsNe",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.355072Z",
          "iopub.execute_input": "2025-02-10T14:34:37.355336Z",
          "iopub.status.idle": "2025-02-10T14:34:37.374248Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.355316Z",
          "shell.execute_reply": "2025-02-10T14:34:37.373492Z"
        }
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract individual dataset zips\n",
        "def extract_inner_zips():\n",
        "    datasets_path = os.path.join(unzip_dir, \"KeywordExtractor-Datasets-master/datasets\")\n",
        "    for file in os.listdir(datasets_path):\n",
        "        if file.endswith(\".zip\"):\n",
        "            zip_path = os.path.join(datasets_path, file)\n",
        "            extract_path = os.path.join(datasets_path, file.replace(\".zip\", \"\"))\n",
        "            if not os.path.exists(extract_path):\n",
        "                print(f\"Extracting {file}...\")\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_path)\n",
        "                print(f\"Extracted {file} to {extract_path}.\")"
      ],
      "metadata": {
        "id": "Cb-eAgdqAvWQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.375391Z",
          "iopub.execute_input": "2025-02-10T14:34:37.375689Z",
          "iopub.status.idle": "2025-02-10T14:34:37.389796Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.375662Z",
          "shell.execute_reply": "2025-02-10T14:34:37.389195Z"
        }
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSING WITHOUT SAMPLING\n",
        "# Step 4: Process a specific dataset and convert it into a usable format\n",
        "def process_dataset(dataset_name):\n",
        "    dataset_path = os.path.join(unzip_dir, f\"KeywordExtractor-Datasets-master/datasets/{dataset_name}/{dataset_name}\")\n",
        "\n",
        "    # Check if dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Dataset {dataset_name} not found.\")\n",
        "        return None\n",
        "\n",
        "    docs_folder = os.path.join(dataset_path, \"docsutf8\")\n",
        "    keys_folder = os.path.join(dataset_path, \"keys\")\n",
        "\n",
        "    if not os.path.exists(docs_folder) or not os.path.exists(keys_folder):\n",
        "        print(f\"Required folders (docsutf8, keys) are missing in {dataset_name}.\")\n",
        "        return None\n",
        "\n",
        "    # Load documents and keywords\n",
        "    print(f\"Processing dataset: {dataset_name}\")\n",
        "    documents = []\n",
        "    keywords = []\n",
        "\n",
        "    for doc_file in sorted(os.listdir(docs_folder)):\n",
        "        doc_path = os.path.join(docs_folder, doc_file)\n",
        "        if doc_file.endswith(\".txt\"):\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                documents.append(f.read().strip())\n",
        "\n",
        "    for key_file in sorted(os.listdir(keys_folder)):\n",
        "        key_path = os.path.join(keys_folder, key_file)\n",
        "        if key_file.endswith(\".key\"):\n",
        "            with open(key_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                keywords.append(f.read().strip().split(','))\n",
        "\n",
        "    # Combine documents and keywords into a DataFrame\n",
        "    data = pd.DataFrame({\"document\": documents, \"keywords\": keywords})\n",
        "    return data"
      ],
      "metadata": {
        "id": "NohTRCgrBBTb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.391101Z",
          "iopub.execute_input": "2025-02-10T14:34:37.391470Z",
          "iopub.status.idle": "2025-02-10T14:34:37.405219Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.391450Z",
          "shell.execute_reply": "2025-02-10T14:34:37.404552Z"
        }
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSING WITH SAMPLING\n",
        "\n",
        "def process_dataset(dataset_name, sample_fraction=0.2, random_seed=64):\n",
        "    dataset_path = os.path.join(unzip_dir, f\"KeywordExtractor-Datasets-master/datasets/{dataset_name}/{dataset_name}\")\n",
        "\n",
        "    # Check if dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Dataset {dataset_name} not found.\")\n",
        "        return None\n",
        "\n",
        "    docs_folder = os.path.join(dataset_path, \"docsutf8\")\n",
        "    keys_folder = os.path.join(dataset_path, \"keys\")\n",
        "\n",
        "    if not os.path.exists(docs_folder) or not os.path.exists(keys_folder):\n",
        "        print(f\"Required folders (docsutf8, keys) are missing in {dataset_name}.\")\n",
        "        return None\n",
        "\n",
        "    # Get list of all document files\n",
        "    doc_files = sorted([f for f in os.listdir(docs_folder) if f.endswith(\".txt\")])\n",
        "\n",
        "    # Set random seed and sample file indices\n",
        "    np.random.seed(random_seed)\n",
        "    sample_size = int(len(doc_files) * sample_fraction)\n",
        "    sampled_indices = np.random.choice(len(doc_files), size=sample_size, replace=False)\n",
        "\n",
        "    print(f\"Original dataset size: {len(doc_files)}\")\n",
        "    print(f\"Sampled dataset size: {sample_size}\")\n",
        "\n",
        "    # Load only sampled documents and their corresponding keywords\n",
        "    documents = []\n",
        "    keywords = []\n",
        "\n",
        "    for idx in sorted(sampled_indices):\n",
        "        # Get document\n",
        "        doc_file = doc_files[idx]\n",
        "        doc_path = os.path.join(docs_folder, doc_file)\n",
        "        with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            documents.append(f.read().strip())\n",
        "\n",
        "        # Get corresponding keywords\n",
        "        key_file = doc_file.replace('.txt', '.key')\n",
        "        key_path = os.path.join(keys_folder, key_file)\n",
        "        with open(key_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            keywords.append(f.read().strip().split(','))\n",
        "\n",
        "    # Combine documents and keywords into a DataFrame\n",
        "    data = pd.DataFrame({\"document\": documents, \"keywords\": keywords})\n",
        "    return data"
      ],
      "metadata": {
        "id": "0BYbwaRAUwUU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.406153Z",
          "iopub.execute_input": "2025-02-10T14:34:37.406430Z",
          "iopub.status.idle": "2025-02-10T14:34:37.426153Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.406411Z",
          "shell.execute_reply": "2025-02-10T14:34:37.425379Z"
        }
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Save processed data to a CSV file\n",
        "def save_to_csv(data, output_path):\n",
        "    print(f\"Saving processed data to {output_path}...\")\n",
        "    data.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "    print(\"Data saved successfully.\")"
      ],
      "metadata": {
        "id": "42uv-e45BJEU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:37.427401Z",
          "iopub.execute_input": "2025-02-10T14:34:37.427656Z",
          "iopub.status.idle": "2025-02-10T14:34:37.444653Z",
          "shell.execute_reply.started": "2025-02-10T14:34:37.427637Z",
          "shell.execute_reply": "2025-02-10T14:34:37.443985Z"
        }
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    download_datasets()\n",
        "    extract_datasets()\n",
        "    extract_inner_zips()\n",
        "\n",
        "    # Example: Process the \"Krapivin2009\" dataset\n",
        "    dataset_name = \"Krapivin2009\"\n",
        "    processed_data = process_dataset(dataset_name)\n",
        "\n",
        "    if processed_data is not None:\n",
        "        output_csv = f\"{dataset_name}_processed.csv\"\n",
        "        save_to_csv(processed_data, output_csv)\n",
        "        #in the format documents,keywords\n",
        "        #keywords are separated either by commas or \\n\n",
        "\n",
        "    # Clean up downloaded zip file\n",
        "    if os.path.exists(local_zip_path):\n",
        "        os.remove(local_zip_path)\n",
        "        print(\"Cleaned up temporary files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3398ac-0a8b-4b20-f53b-039384c3ea2f",
        "id": "rjardzbIk5g_",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:34:45.324862Z",
          "iopub.execute_input": "2025-02-10T14:34:45.325159Z",
          "iopub.status.idle": "2025-02-10T14:34:56.046157Z",
          "shell.execute_reply.started": "2025-02-10T14:34:45.325137Z",
          "shell.execute_reply": "2025-02-10T14:34:56.045402Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading datasets...\n",
            "Datasets downloaded successfully.\n",
            "Extracting datasets...\n",
            "Datasets extracted successfully.\n",
            "Original dataset size: 2304\n",
            "Sampled dataset size: 460\n",
            "Saving processed data to Krapivin2009_processed.csv...\n",
            "Data saved successfully.\n",
            "Cleaned up temporary files.\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "file_path = \"./Krapivin2009_processed.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\"\"\"\n",
        "# Extract documents and keywords\n",
        "documents = data['document']  # This is a pandas Series of text documents\n",
        "keywords = data['keywords']  # This is a pandas Series of keyword strings\n",
        "\"\"\"\n",
        "\n",
        "# Extract documents and keywords\n",
        "# save them in pandas dataframe in the 'text' column\n",
        "documents = pd.DataFrame({'text': data['document']})\n",
        "keywords = pd.DataFrame({'text': data['keywords']})\n",
        "\n",
        "# Convert keywords from string representation to Python lists\n",
        "keywords['text'] = keywords['text'].apply(lambda x: eval(x))  # Use `eval` to parse strings into lists if necessary\n",
        "#Now it is a list containing only one long string in the format ['Keyword1\\nKeyword2\\nKeyword3....']\n",
        "keywords['text'] = keywords['text'].apply(lambda x: x[0].split('\\n'))\n",
        "#now keywords is a list of lowercase strings"
      ],
      "metadata": {
        "id": "Dao9TY5sk9xm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:36:06.452445Z",
          "iopub.execute_input": "2025-02-10T14:36:06.452751Z",
          "iopub.status.idle": "2025-02-10T14:36:06.648544Z",
          "shell.execute_reply.started": "2025-02-10T14:36:06.452729Z",
          "shell.execute_reply": "2025-02-10T14:36:06.647913Z"
        }
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART-OF-SPEECH TAGGING [documents]"
      ],
      "metadata": {
        "id": "fNYsnAtSXXS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "qAX5sXkTXWWv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:36:15.201319Z",
          "iopub.execute_input": "2025-02-10T14:36:15.201593Z",
          "iopub.status.idle": "2025-02-10T14:36:23.376336Z",
          "shell.execute_reply.started": "2025-02-10T14:36:15.201573Z",
          "shell.execute_reply": "2025-02-10T14:36:23.375453Z"
        }
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "# the function compiute part-of-speech tagging using spacy on a text and return a list of pair (word, pos)\n",
        "\n",
        "def pos_tag_document(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Store and return tagged words with their parts of speech\n",
        "    # each token object has a token.texta and a token.pos_ attribute here are saved in a list of pairs\n",
        "    tagged_words = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    return tagged_words"
      ],
      "metadata": {
        "id": "Ar3mXJ0rVCrH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:36:26.231330Z",
          "iopub.execute_input": "2025-02-10T14:36:26.231798Z",
          "iopub.status.idle": "2025-02-10T14:36:26.235752Z",
          "shell.execute_reply.started": "2025-02-10T14:36:26.231771Z",
          "shell.execute_reply": "2025-02-10T14:36:26.234924Z"
        }
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "\n",
        "start = time.time()\n",
        "documents['pos'] = documents['text'].apply(pos_tag_document)\n",
        "end = time.time()\n",
        "print(f'part-of-speech tagging of the documents {end-start}')\n",
        "\n",
        "\"\"\"Time to tag documents: 909.2577395439148\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "vJNbYn_svzNY",
        "outputId": "00922f1b-499a-4fd5-9b53-c94ec23cd7e9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T14:36:42.382661Z",
          "iopub.execute_input": "2025-02-10T14:36:42.382996Z",
          "iopub.status.idle": "2025-02-10T14:51:51.644773Z",
          "shell.execute_reply.started": "2025-02-10T14:36:42.382941Z",
          "shell.execute_reply": "2025-02-10T14:51:51.643846Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-of-speech tagging of the documents 954.178055524826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Time to tag documents: 909.2577395439148'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART-OF-SPEECH ASSOCIATION [keywords]"
      ],
      "metadata": {
        "id": "Ss3yoHv5bG3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "import re"
      ],
      "metadata": {
        "id": "Y40hqthnb4-Q",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T15:10:13.450402Z",
          "iopub.execute_input": "2025-02-10T15:10:13.450695Z",
          "iopub.status.idle": "2025-02-10T15:10:13.454179Z",
          "shell.execute_reply.started": "2025-02-10T15:10:13.450672Z",
          "shell.execute_reply": "2025-02-10T15:10:13.453418Z"
        }
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # Load Spacy model for POS tagging and lemmatization\n",
        "\n",
        "def pos_of_keywords_method_one(keywords, text_pos, window_size=5):\n",
        "    \"\"\"\n",
        "    Identify the most common Part-of-Speech (POS) tag for each word in a multi-word keyword\n",
        "    by analyzing its occurrences in a given text. The function checks if all lemmatized parts of a\n",
        "    keyword appear within a dynamically adjusted window in the text and assigns the most\n",
        "    frequent POS tags accordingly.\n",
        "    If a keyword or part of it is not found in a relevant context, the corresponding POS\n",
        "    position is left empty (None).\n",
        "\n",
        "    Args:\n",
        "    - keywords (list of str): The keywords to find in the text.\n",
        "    - text_pos (list of tuples): A list of (word, POS tag) tuples.\n",
        "    - window_size (int): The default context window size around words.\n",
        "\n",
        "    Returns:\n",
        "    - list of tuples: Each tuple contains the most frequent POS tag(s)\n",
        "                      for each word in a keyword. If no match is found, returns None.\n",
        "\n",
        "    \"\"\"\n",
        "    key_pos = []\n",
        "\n",
        "    # extract words and their POS tags separately, converting words to lowercase\n",
        "    text_tokens = [token.lower() for token, _ in text_pos]\n",
        "    text_pos_tags = [pos for _, pos in text_pos]\n",
        "\n",
        "    # Precompute lemmas for the entire text (word-by-word approach)\n",
        "    text_lemmas = {word: nlp(word)[0].lemma_ for word in set(text_tokens)}\n",
        "    lemmatized_text = [text_lemmas[word] for word in text_tokens] # lemmatized version of text tokens\n",
        "\n",
        "    for key in keywords:\n",
        "        key_tokens = re.findall(r'\\w+|[-/()]', key.lower())  # tokenize the keyword, keeping hyphens (-) and slashes (/) as separate tokens\n",
        "        key_lemmas = [text_lemmas.get(word, word) for word in key_tokens]  # lemmatize keyword tokens\n",
        "\n",
        "        dynamic_window_size = max(window_size, len(key_tokens) * 2) # adjust window size based on keyword length\n",
        "        keyword_word_pos = [] #store POS tags for each word in the keyword\n",
        "\n",
        "        # Store word positions for faster lookup\n",
        "        word_positions = {word: [] for word in key_tokens}\n",
        "        for i, word in enumerate(text_tokens):\n",
        "            if word in word_positions:\n",
        "                word_positions[word].append(i)\n",
        "\n",
        "        # Search for each word in the keyword separately\n",
        "        for token in key_tokens:\n",
        "            token_pos_counts = Counter()\n",
        "            for i in word_positions.get(token, []):  # direct lookup of positions\n",
        "                # Look at words in the surrounding context window\n",
        "                start, end = max(0, i - dynamic_window_size), min(len(text_tokens), i + dynamic_window_size + 1)\n",
        "\n",
        "                context_lemmas = set(lemmatized_text[start:end])  # convert the context words to their lemmas for matching\n",
        "\n",
        "                # Get remaining lemmas of the keyword (excluding the current word)\n",
        "                current_lemma = text_lemmas.get(token, token)\n",
        "                remaining_keyword_lemmas = [kw_lemma for kw_lemma in key_lemmas if kw_lemma != current_lemma]\n",
        "\n",
        "                # Check if all other keyword lemmas exist in the context window\n",
        "                if all(kw_lemma in context_lemmas for kw_lemma in remaining_keyword_lemmas):\n",
        "                    token_pos_counts[text_pos_tags[i]] += 1  # record the POS tag for this token\n",
        "\n",
        "            # Determine the most frequent POS tag for the token\n",
        "            if token_pos_counts:\n",
        "                most_common_pos = token_pos_counts.most_common(1)[0][0]\n",
        "                keyword_word_pos.append(most_common_pos)\n",
        "            else:\n",
        "                keyword_word_pos.append(None) #no match found\n",
        "\n",
        "        key_pos.append(tuple(keyword_word_pos))\n",
        "\n",
        "\n",
        "    return key_pos"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T15:08:53.398775Z",
          "iopub.execute_input": "2025-02-10T15:08:53.399183Z",
          "iopub.status.idle": "2025-02-10T15:08:54.020622Z",
          "shell.execute_reply.started": "2025-02-10T15:08:53.399148Z",
          "shell.execute_reply": "2025-02-10T15:08:54.019815Z"
        },
        "id": "NmV11mV-ES8J",
        "outputId": "de058ffe-9d8e-4bf8-c9c9-fb57a2bc8a56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_of_keywords_method_two(keywords, text_pos):\n",
        "\n",
        "    \"\"\"\n",
        "    Identify the most common POS tag for each word in a multi-word keyword\n",
        "    based on its occurrences in a given text with POS tagging.\n",
        "\n",
        "    Args:\n",
        "        - keywords (list of str): The keywords to find in the text.\n",
        "        - text_pos (list of tuples): A list of (word, POS tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "        - list of tuples: Each tuple contains the most frequent POS tag(s)\n",
        "          for each word in a keyword. If no match is found, returns None.\n",
        "    \"\"\"\n",
        "\n",
        "    key_pos = []\n",
        "\n",
        "    # extract lowercase words and POS tags from text for comparison\n",
        "    text = [text.lower() for text, _ in text_pos]\n",
        "    pos = [pos for _, pos in text_pos]\n",
        "\n",
        "\n",
        "    for key in keywords:\n",
        "        key_tokens = key.lower().split()\n",
        "        word_pos = []\n",
        "\n",
        "        for word in key_tokens: #process each word separately\n",
        "\n",
        "            # Find all occurrences of the word\n",
        "            occurrences = []\n",
        "            for i in range(len(text)):\n",
        "              if text[i] == word:\n",
        "                occurrences.append(pos[i]) #store corresponding POS tag\n",
        "\n",
        "            # Determine the most common POS tag for this word\n",
        "            if occurrences:\n",
        "                most_common_pos = Counter(occurrences).most_common(1)[0][0]\n",
        "                word_pos.append(most_common_pos)\n",
        "            else:\n",
        "                word_pos.append(None)\n",
        "\n",
        "        key_pos.append(tuple(word_pos))\n",
        "\n",
        "    return(key_pos)"
      ],
      "metadata": {
        "id": "NTEA_25saDfH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T15:08:59.246538Z",
          "iopub.execute_input": "2025-02-10T15:08:59.246827Z",
          "iopub.status.idle": "2025-02-10T15:08:59.252517Z",
          "shell.execute_reply.started": "2025-02-10T15:08:59.246807Z",
          "shell.execute_reply": "2025-02-10T15:08:59.251636Z"
        }
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "#METHOD ONE\n",
        "# execute pos_of_keywords on every row of the dataframes using method one\n",
        "\n",
        "start = time.time()\n",
        "keywords['pos_one'] = keywords.apply(lambda x: pos_of_keywords_method_one(x['text'], documents['pos'][x.name]), axis=1)\n",
        "end = time.time()\n",
        "print(f'part-of-speech tagging with method one of the keywords {end-start}')\n",
        "\"\"\"Time to execute method one: 2768s\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PmQt-dyW0aod",
        "outputId": "a40af37f-043e-43c3-9976-1a309765fea5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T17:18:52.361587Z",
          "iopub.execute_input": "2025-02-10T17:18:52.361800Z",
          "iopub.status.idle": "2025-02-10T17:36:57.835208Z",
          "shell.execute_reply.started": "2025-02-10T17:18:52.361782Z",
          "shell.execute_reply": "2025-02-10T17:36:57.833315Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-of-speech tagging with method one of the keywords 2768.3002519607544\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Time to execute method one: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "#METHOD TWO\n",
        "# execute pos_of_keywords on every row of the dataframes using method two\n",
        "\n",
        "start = time.time()\n",
        "keywords['pos_two'] = keywords.apply(lambda x: pos_of_keywords_method_two(x['text'], documents['pos'][x.name]), axis=1)\n",
        "end = time.time()\n",
        "print(f'part-of-speech tagging with method two of the keywords {end-start}')\n",
        "\"\"\"Time to execute method two: 3s\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T16:35:58.687703Z",
          "iopub.execute_input": "2025-02-10T16:35:58.688093Z",
          "iopub.status.idle": "2025-02-10T17:18:52.360611Z",
          "shell.execute_reply.started": "2025-02-10T16:35:58.688067Z",
          "shell.execute_reply": "2025-02-10T17:18:52.359905Z"
        },
        "id": "W_t8aRbXES8M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fa4dd968-6388-44d7-8a52-fe52bd3c27af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-of-speech tagging with method two of the keywords 3.185098171234131\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Time to execute method two: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_pos_sequences(pos_list, n=50):\n",
        "  most_common_pos = Counter(pos_list).most_common(n)\n",
        "  #discard those with None elements and those with lenght > 3\n",
        "  return [mcs[0] for mcs in most_common_pos if all(item is not None for item in mcs[0]) and len(mcs[0]) <= 3]\n"
      ],
      "metadata": {
        "id": "l5kDcuPg1-PP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T17:50:02.710482Z",
          "iopub.execute_input": "2025-02-10T17:50:02.710827Z",
          "iopub.status.idle": "2025-02-10T17:50:02.716883Z",
          "shell.execute_reply.started": "2025-02-10T17:50:02.710797Z",
          "shell.execute_reply": "2025-02-10T17:50:02.716110Z"
        }
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "# have all the keyword pos sequence in a single list\n",
        "flattened_method_one = tuple(keywords['pos_one'].explode().tolist())\n",
        "\n",
        "accepted_pos_sequences_method_one = most_common_pos_sequences(flattened_method_one)\n",
        "print(len(accepted_pos_sequences_method_one))\n",
        "accepted_pos_sequences_method_one"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T18:10:58.599376Z",
          "iopub.execute_input": "2025-02-10T18:10:58.599689Z",
          "iopub.status.idle": "2025-02-10T18:10:58.608271Z",
          "shell.execute_reply.started": "2025-02-10T18:10:58.599663Z",
          "shell.execute_reply": "2025-02-10T18:10:58.607577Z"
        },
        "id": "CdV_RRA0ES8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4ed2ca-a0c7-4f36-e65a-160c3a3c741e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NOUN', 'NOUN'),\n",
              " ('ADJ', 'NOUN'),\n",
              " ('NOUN',),\n",
              " ('PROPN', 'NOUN'),\n",
              " ('PROPN', 'PROPN'),\n",
              " ('ADJ', 'NOUN', 'NOUN'),\n",
              " ('PROPN',),\n",
              " ('VERB', 'NOUN'),\n",
              " ('NOUN', 'VERB'),\n",
              " ('NOUN', 'NOUN', 'NOUN'),\n",
              " ('VERB',),\n",
              " ('ADJ', 'ADJ', 'NOUN'),\n",
              " ('PROPN', 'PROPN', 'PROPN'),\n",
              " ('NOUN', 'PROPN'),\n",
              " ('VERB', 'NOUN', 'NOUN'),\n",
              " ('ADJ', 'PROPN'),\n",
              " ('ADJ',),\n",
              " ('NOUN', 'ADJ', 'NOUN'),\n",
              " ('NOUN', 'ADP', 'NOUN'),\n",
              " ('NOUN', 'PUNCT', 'NOUN'),\n",
              " ('PROPN', 'NOUN', 'NOUN'),\n",
              " ('ADJ', 'NOUN', 'VERB'),\n",
              " ('PROPN', 'PROPN', 'NOUN'),\n",
              " ('ADJ', 'PROPN', 'NOUN'),\n",
              " ('NOUN', 'PUNCT', 'VERB'),\n",
              " ('PROPN', 'VERB'),\n",
              " ('PROPN', 'ADJ', 'NOUN'),\n",
              " ('NOUN', 'PROPN', 'NOUN'),\n",
              " ('ADJ', 'VERB'),\n",
              " ('PROPN', 'ADP', 'PROPN')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_method_two = tuple(keywords['pos_two'].explode().tolist())\n",
        "\n",
        "accepted_pos_sequences_method_two = most_common_pos_sequences(flattened_method_two)\n",
        "accepted_pos_sequences_method_two"
      ],
      "metadata": {
        "trusted": true,
        "id": "f1JOefMmES8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1d52f9-6e80-47f1-a36e-decd055abd2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NOUN', 'NOUN'),\n",
              " ('ADJ', 'NOUN'),\n",
              " ('NOUN',),\n",
              " ('ADJ', 'NOUN', 'NOUN'),\n",
              " ('PROPN', 'NOUN'),\n",
              " ('PROPN',),\n",
              " ('VERB', 'NOUN'),\n",
              " ('NOUN', 'VERB'),\n",
              " ('NOUN', 'NOUN', 'NOUN'),\n",
              " ('NOUN', 'PROPN'),\n",
              " ('VERB',),\n",
              " ('ADJ', 'ADJ', 'NOUN'),\n",
              " ('PROPN', 'PROPN'),\n",
              " ('ADJ', 'PROPN'),\n",
              " ('VERB', 'NOUN', 'NOUN'),\n",
              " ('NOUN', 'ADP', 'NOUN'),\n",
              " ('ADJ',),\n",
              " ('NOUN', 'ADJ', 'NOUN'),\n",
              " ('ADJ', 'VERB'),\n",
              " ('PROPN', 'NOUN', 'NOUN'),\n",
              " ('ADJ', 'PROPN', 'NOUN'),\n",
              " ('PROPN', 'ADJ', 'NOUN'),\n",
              " ('NOUN', 'VERB', 'NOUN'),\n",
              " ('ADJ', 'NOUN', 'VERB'),\n",
              " ('NOUN', 'PROPN', 'NOUN'),\n",
              " ('PROPN', 'VERB'),\n",
              " ('VERB', 'PROPN'),\n",
              " ('VERB', 'ADJ', 'NOUN'),\n",
              " ('ADJ', 'ADJ'),\n",
              " ('NOUN', 'ADJ'),\n",
              " ('PROPN', 'NOUN', 'PROPN'),\n",
              " ('X', 'X', 'NOUN'),\n",
              " ('NOUN', 'NOUN', 'VERB')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "source": [
        "selection1 = accepted_pos_sequences_method_one[:20]\n",
        "print(selection1)\n",
        "\n",
        "selection2 = accepted_pos_sequences_method_two[:20]\n",
        "print(selection2)"
      ],
      "metadata": {
        "id": "cNt37ggrLfIv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T17:57:15.696720Z",
          "iopub.execute_input": "2025-02-10T17:57:15.697054Z",
          "iopub.status.idle": "2025-02-10T17:57:15.701546Z",
          "shell.execute_reply.started": "2025-02-10T17:57:15.697029Z",
          "shell.execute_reply": "2025-02-10T17:57:15.700750Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501b1969-2f82-425d-d69b-fde5d6d86cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NOUN', 'NOUN'), ('ADJ', 'NOUN'), ('NOUN',), ('PROPN', 'NOUN'), ('PROPN', 'PROPN'), ('ADJ', 'NOUN', 'NOUN'), ('PROPN',), ('VERB', 'NOUN'), ('NOUN', 'VERB'), ('NOUN', 'NOUN', 'NOUN'), ('VERB',), ('ADJ', 'ADJ', 'NOUN'), ('PROPN', 'PROPN', 'PROPN'), ('NOUN', 'PROPN'), ('VERB', 'NOUN', 'NOUN'), ('ADJ', 'PROPN'), ('ADJ',), ('NOUN', 'ADJ', 'NOUN'), ('NOUN', 'ADP', 'NOUN'), ('NOUN', 'PUNCT', 'NOUN')]\n",
            "[('NOUN', 'NOUN'), ('ADJ', 'NOUN'), ('NOUN',), ('ADJ', 'NOUN', 'NOUN'), ('PROPN', 'NOUN'), ('PROPN',), ('VERB', 'NOUN'), ('NOUN', 'VERB'), ('NOUN', 'NOUN', 'NOUN'), ('NOUN', 'PROPN'), ('VERB',), ('ADJ', 'ADJ', 'NOUN'), ('PROPN', 'PROPN'), ('ADJ', 'PROPN'), ('VERB', 'NOUN', 'NOUN'), ('NOUN', 'ADP', 'NOUN'), ('ADJ',), ('NOUN', 'ADJ', 'NOUN'), ('ADJ', 'VERB'), ('PROPN', 'NOUN', 'NOUN')]\n"
          ]
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save combinations"
      ],
      "metadata": {
        "id": "jrSuZwuBES8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "o0GuOzm5L8sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb020bbc-9715-4176-8b27-3f42efc29e65"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir_one = \"keywords_combinations_method_one.csv\"\n",
        "with open(output_dir_one, \"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(accepted_pos_sequences_method_one)\n",
        "\n",
        "%cd  /content/\n",
        "\n",
        "files.download(output_dir_one)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-10T17:59:13.805734Z",
          "iopub.execute_input": "2025-02-10T17:59:13.806100Z",
          "iopub.status.idle": "2025-02-10T17:59:13.814498Z",
          "shell.execute_reply.started": "2025-02-10T17:59:13.806075Z",
          "shell.execute_reply": "2025-02-10T17:59:13.813780Z"
        },
        "id": "bW7EF5r-ES8P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e50b9140-b979-4416-8ea5-caa438c0e0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_42c9aa3e-ace1-45bd-b8b2-b5a35b884532\", \"keywords_combinations_method_one.csv\", 397)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir_two = \"keywords_combinations_method_two.csv\"\n",
        "with open(output_dir_two, \"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(accepted_pos_sequences_method_two)\n",
        "\n",
        "%cd  /content/\n",
        "\n",
        "files.download(output_dir_two)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7LKMNPGVES8P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bd2f650-9cb5-4ee8-b6c9-fa1f35caa570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_971e955a-18b5-402d-9ec1-2636d42329a5\", \"keywords_combinations_method_two.csv\", 415)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 44
    }
  ]
}