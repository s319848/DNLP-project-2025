{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATABASE EXTRACTION and PROCESSING"
      ],
      "metadata": {
        "id": "eLBEqntdPk62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Oj3Rk0eFVDRz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hP9slT9JPENL"
      },
      "outputs": [],
      "source": [
        "# Define the URL for the dataset repository and the local storage directory\n",
        "data_url = \"https://github.com/LIAAD/KeywordExtractor-Datasets/archive/refs/heads/master.zip\"\n",
        "local_zip_path = \"datasets.zip\"\n",
        "unzip_dir = \"KeywordExtractor-Datasets\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download the dataset repository\n",
        "def download_datasets():\n",
        "    print(\"Downloading datasets...\")\n",
        "    response = requests.get(data_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_zip_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(\"Datasets downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download datasets. Status code: {response.status_code}\")\n",
        "        exit(1)"
      ],
      "metadata": {
        "id": "m-nw69U-AmwF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract the downloaded zip file\n",
        "def extract_datasets():\n",
        "    print(\"Extracting datasets...\")\n",
        "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(unzip_dir)\n",
        "    print(\"Datasets extracted successfully.\")"
      ],
      "metadata": {
        "id": "t9odQOW3AsNe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract individual dataset zips\n",
        "def extract_inner_zips():\n",
        "    datasets_path = os.path.join(unzip_dir, \"KeywordExtractor-Datasets-master/datasets\")\n",
        "    for file in os.listdir(datasets_path):\n",
        "        if file.endswith(\".zip\"):\n",
        "            zip_path = os.path.join(datasets_path, file)\n",
        "            extract_path = os.path.join(datasets_path, file.replace(\".zip\", \"\"))\n",
        "            if not os.path.exists(extract_path):\n",
        "                print(f\"Extracting {file}...\")\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_path)\n",
        "                print(f\"Extracted {file} to {extract_path}.\")"
      ],
      "metadata": {
        "id": "Cb-eAgdqAvWQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSING WITHOUT SAMPLING\n",
        "# Step 4: Process a specific dataset and convert it into a usable format\n",
        "def process_dataset(dataset_name):\n",
        "    dataset_path = os.path.join(unzip_dir, f\"KeywordExtractor-Datasets-master/datasets/{dataset_name}/{dataset_name}\")\n",
        "\n",
        "    # Check if dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Dataset {dataset_name} not found.\")\n",
        "        return None\n",
        "\n",
        "    docs_folder = os.path.join(dataset_path, \"docsutf8\")\n",
        "    keys_folder = os.path.join(dataset_path, \"keys\")\n",
        "\n",
        "    if not os.path.exists(docs_folder) or not os.path.exists(keys_folder):\n",
        "        print(f\"Required folders (docsutf8, keys) are missing in {dataset_name}.\")\n",
        "        return None\n",
        "\n",
        "    # Load documents and keywords\n",
        "    print(f\"Processing dataset: {dataset_name}\")\n",
        "    documents = []\n",
        "    keywords = []\n",
        "\n",
        "    for doc_file in sorted(os.listdir(docs_folder)):\n",
        "        doc_path = os.path.join(docs_folder, doc_file)\n",
        "        if doc_file.endswith(\".txt\"):\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                documents.append(f.read().strip())\n",
        "\n",
        "    for key_file in sorted(os.listdir(keys_folder)):\n",
        "        key_path = os.path.join(keys_folder, key_file)\n",
        "        if key_file.endswith(\".key\"):\n",
        "            with open(key_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                keywords.append(f.read().strip().split(','))\n",
        "\n",
        "    # Combine documents and keywords into a DataFrame\n",
        "    data = pd.DataFrame({\"document\": documents, \"keywords\": keywords})\n",
        "    return data"
      ],
      "metadata": {
        "id": "NohTRCgrBBTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSING WITH SAMPLING\n",
        "\n",
        "def process_dataset(dataset_name, sample_fraction=0.1, random_seed=64):\n",
        "    dataset_path = os.path.join(unzip_dir, f\"KeywordExtractor-Datasets-master/datasets/{dataset_name}/{dataset_name}\")\n",
        "\n",
        "    # Check if dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Dataset {dataset_name} not found.\")\n",
        "        return None\n",
        "\n",
        "    docs_folder = os.path.join(dataset_path, \"docsutf8\")\n",
        "    keys_folder = os.path.join(dataset_path, \"keys\")\n",
        "\n",
        "    if not os.path.exists(docs_folder) or not os.path.exists(keys_folder):\n",
        "        print(f\"Required folders (docsutf8, keys) are missing in {dataset_name}.\")\n",
        "        return None\n",
        "\n",
        "    # Get list of all document files\n",
        "    doc_files = sorted([f for f in os.listdir(docs_folder) if f.endswith(\".txt\")])\n",
        "\n",
        "    # Set random seed and sample file indices\n",
        "    np.random.seed(random_seed)\n",
        "    sample_size = int(len(doc_files) * sample_fraction)\n",
        "    sampled_indices = np.random.choice(len(doc_files), size=sample_size, replace=False)\n",
        "\n",
        "    print(f\"Original dataset size: {len(doc_files)}\")\n",
        "    print(f\"Sampled dataset size: {sample_size}\")\n",
        "\n",
        "    # Load only sampled documents and their corresponding keywords\n",
        "    documents = []\n",
        "    keywords = []\n",
        "\n",
        "    for idx in sorted(sampled_indices):\n",
        "        # Get document\n",
        "        doc_file = doc_files[idx]\n",
        "        doc_path = os.path.join(docs_folder, doc_file)\n",
        "        with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            documents.append(f.read().strip())\n",
        "\n",
        "        # Get corresponding keywords\n",
        "        key_file = doc_file.replace('.txt', '.key')\n",
        "        key_path = os.path.join(keys_folder, key_file)\n",
        "        with open(key_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            keywords.append(f.read().strip().split(','))\n",
        "\n",
        "    # Combine documents and keywords into a DataFrame\n",
        "    data = pd.DataFrame({\"document\": documents, \"keywords\": keywords})\n",
        "    return data"
      ],
      "metadata": {
        "id": "0BYbwaRAUwUU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Save processed data to a CSV file\n",
        "def save_to_csv(data, output_path):\n",
        "    print(f\"Saving processed data to {output_path}...\")\n",
        "    data.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "    print(\"Data saved successfully.\")"
      ],
      "metadata": {
        "id": "42uv-e45BJEU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    download_datasets()\n",
        "    extract_datasets()\n",
        "    extract_inner_zips()\n",
        "\n",
        "    # Example: Process the \"Krapivin2009\" dataset\n",
        "    dataset_name = \"Krapivin2009\"\n",
        "    processed_data = process_dataset(dataset_name)\n",
        "\n",
        "    if processed_data is not None:\n",
        "        output_csv = f\"{dataset_name}_processed.csv\"\n",
        "        save_to_csv(processed_data, output_csv)\n",
        "        #in the format documents,keywords\n",
        "        #keywords are separated either by commas or \\n\n",
        "\n",
        "    # Clean up downloaded zip file\n",
        "    if os.path.exists(local_zip_path):\n",
        "        os.remove(local_zip_path)\n",
        "        print(\"Cleaned up temporary files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07532ad-b9fa-40c4-d244-1e9e27b2bbfd",
        "collapsed": true,
        "id": "rjardzbIk5g_"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading datasets...\n",
            "Datasets downloaded successfully.\n",
            "Extracting datasets...\n",
            "Datasets extracted successfully.\n",
            "Original dataset size: 2304\n",
            "Sampled dataset size: 230\n",
            "Saving processed data to Krapivin2009_processed.csv...\n",
            "Data saved successfully.\n",
            "Cleaned up temporary files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "file_path = \"./Krapivin2009_processed.csv\"  # Update with the correct path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\"\"\"\n",
        "# Extract documents and keywords\n",
        "documents = data['document']  # This is a pandas Series of text documents\n",
        "keywords = data['keywords']  # This is a pandas Series of keyword strings\n",
        "\"\"\"\n",
        "\n",
        "# Extract documents and keywords\n",
        "# save them in pandas dataframe in the 'text' column\n",
        "documents = pd.DataFrame({'text': data['document']})\n",
        "keywords = pd.DataFrame({'text': data['keywords']})\n",
        "\n",
        "# Convert keywords from string representation to Python lists\n",
        "keywords['text'] = keywords['text'].apply(lambda x: eval(x))  # Use `eval` to parse strings into lists if necessary\n",
        "#Now it is a list containing only one long string in the format ['Keyword1\\nKeyword2\\nKeyword3....']\n",
        "# keywords = keywords.apply(lambda x: x[0].lower()) #only one element in the list\n",
        "#Now x is a string lowercase\n",
        "keywords['text'] = keywords['text'].apply(lambda x: x[0].split('\\n'))\n",
        "#now keywords is a list of lowercase strings"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Dao9TY5sk9xm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART-OF-SPEECH TAGGING [documents]"
      ],
      "metadata": {
        "id": "fNYsnAtSXXS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "qAX5sXkTXWWv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the function compiute part-of-speech tagging using spacy on a text and return a list of pair (word, pos)\n",
        "\n",
        "def pos_tag_document(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Store and return tagged words with their parts of speech\n",
        "    # each token object has a token.texta and a token.pos_ attribute here are saved in a list of pairs\n",
        "    tagged_words = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    return tagged_words"
      ],
      "metadata": {
        "id": "Ar3mXJ0rVCrH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "\n",
        "start = time.time()\n",
        "documents['pos'] = documents['text'].apply(pos_tag_document)\n",
        "end = time.time()\n",
        "print(f'part-of-speech tagging of the documents {end-start}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJNbYn_svzNY",
        "outputId": "f8874c85-2b9a-4b96-b3dc-bdffa8661fc8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-of-speech tagging of the documents 596.9443778991699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART-OF-SPEECH ASSOCIATION [keywords]"
      ],
      "metadata": {
        "id": "Ss3yoHv5bG3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "import re"
      ],
      "metadata": {
        "id": "Y40hqthnb4-Q"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it recive the pos tagging of a document and the list of keyword of that document and return\n",
        "# the pos of the keywords obtained by collecting the pos taggs of all the occurrence of the keyword in the\n",
        "# text and keeping the most common\n",
        "\n",
        "def pos_of_keywords(keywords, text_pos):\n",
        "\n",
        "  key_pos = []\n",
        "  # lower the word of the text to have better comparison\n",
        "  text = [text.lower() for text, _ in text_pos]\n",
        "  pos = [pos for _, pos in text_pos]\n",
        "\n",
        "\n",
        "  for key in keywords:\n",
        "        # PREVIOUS : Split the keywords in single words and lower them for better comparison\n",
        "        # key_tokens = key.lower().split()\n",
        "\n",
        "        # using findall regular exression I split the string and consider \"-\" and \"/\" as a string\n",
        "        # as well sonce the POS includes them\n",
        "        # - and / because are the recurring one in the keyword based on a sample of the kewords not found\n",
        "        key_tokens = re.findall(r'\\w+|[-/()]', key.lower())\n",
        "\n",
        "        # Find all occurrences of the keyword\n",
        "        occurrences = []\n",
        "        for i in range(len(text_pos) - len(key_tokens) + 1):\n",
        "            if text[i:i+len(key_tokens)] == key_tokens:\n",
        "                # Extract POS sequence for this occurrence\n",
        "                pos_sequence = [pos[j] for j in range(i, i+len(key_tokens))]\n",
        "                occurrences.append(tuple(pos_sequence)) # tuple because counter require hushable type\n",
        "\n",
        "        # If occurrences found, use the most common POS sequence\n",
        "        if occurrences:\n",
        "            # Counter collect occurrences and # of occurrence ((('ADJ','ADJ'), 3), ('NOUN', 'ADJ'), 1)\n",
        "            # most_common(n) selects a list of the n most common occurrences\n",
        "            most_common_pos = Counter(occurrences).most_common(1)[0][0]\n",
        "            key_pos.append(most_common_pos)\n",
        "        else:\n",
        "            key_pos.append(None)\n",
        "\n",
        "  return(key_pos)"
      ],
      "metadata": {
        "id": "NTEA_25saDfH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute pos_of_keywords on every row of the dataframes\n",
        "\n",
        "start = time.time()\n",
        "keywords['pos'] = keywords.apply(lambda x: pos_of_keywords(x['text'], documents['pos'][x.name]), axis=1)\n",
        "end = time.time()\n",
        "print(f'part-of-speech tagging of the keywords {end-start}') # by associaton of the keywords to their occurrences in the text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmQt-dyW0aod",
        "outputId": "2d3456c8-bd31-49fa-ebcb-e55a80e3f752"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-of-speech tagging of the keywords 3.2202203273773193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO : decide how many pos sequence to select\n",
        "\n",
        "def most_common_pos_sequences(pos_list, n=50):\n",
        "  most_common_pos = Counter(pos_list).most_common(n)\n",
        "  # print(most_common_pos)\n",
        "  # print(len(most_common_pos))\n",
        "  return [mcs[0] for mcs in most_common_pos if mcs[0] is not None]\n",
        "\n",
        "# have all the keyword pos sequence in a single list\n",
        "flattened = tuple(keywords['pos'].explode().tolist())\n",
        "\n",
        "accepted_pos_sequences = most_common_pos_sequences(flattened)\n",
        "print(accepted_pos_sequences)\n",
        "\n",
        "# Forse poche poarole chiave\n",
        "# Da eliminare quelle da 4\n",
        "# Tutte ragionevoli e ben distribuite da 50"
      ],
      "metadata": {
        "id": "l5kDcuPg1-PP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}